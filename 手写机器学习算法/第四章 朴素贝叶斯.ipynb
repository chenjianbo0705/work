{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList =[['my','dog','has','flea','problems','help','please'],\n",
    "                  ['maybe','not','take','him','to','dog','parl','stupid'],\n",
    "                  ['my','dalmtion','is','so','cute','I','love','him'],\n",
    "                  ['stop','posting','stupid','worthless','garbage'],\n",
    "                  ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "                  ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec=[0,1,0,1,0,1]\n",
    "    return postingList,classVec\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "def setOfWords2vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else :print('the word: %s is not  in my  Vacobulary!' %word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts,listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stop',\n",
       " 'so',\n",
       " 'how',\n",
       " 'my',\n",
       " 'I',\n",
       " 'quit',\n",
       " 'stupid',\n",
       " 'licks',\n",
       " 'is',\n",
       " 'posting',\n",
       " 'ate',\n",
       " 'to',\n",
       " 'problems',\n",
       " 'not',\n",
       " 'him',\n",
       " 'maybe',\n",
       " 'love',\n",
       " 'steak',\n",
       " 'worthless',\n",
       " 'garbage',\n",
       " 'has',\n",
       " 'mr',\n",
       " 'flea',\n",
       " 'buying',\n",
       " 'help',\n",
       " 'dog',\n",
       " 'food',\n",
       " 'parl',\n",
       " 'dalmtion',\n",
       " 'please',\n",
       " 'take',\n",
       " 'cute']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2vec(myVocabList, listOPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2vec(myVocabList, listOPosts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import operator\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDoces = len(trainMatrix)\n",
    "    numWrods = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDoces)\n",
    "    p0Num = ones(numWrods)\n",
    "    p1Num = ones(numWrods)\n",
    "    p0Denom = 2.0; p1Denom = 2.0\n",
    "    for i in range(numTrainDoces):\n",
    "        if trainCategory[i] ==1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom +=sum(trainMatrix[i])\n",
    "        else :\n",
    "            p0Num +=trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)\n",
    "    p0Vect = log(p0Num/p0Denom)\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import operator\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDoces = len(trainMatrix)\n",
    "    numWrods = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDoces)\n",
    "    p0Num = zeros(numWrods)\n",
    "    p1Num = zeros(numWrods)\n",
    "    p0Denom = 2.0; p1Denom = 2.0\n",
    "    for i in range(numTrainDoces):\n",
    "        if trainCategory[i] ==1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom +=sum(trainMatrix[i])\n",
    "        else :\n",
    "            p0Num +=trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = p1Num/p1Denom\n",
    "    p0Vect = p0Num/p0Denom\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts,listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat= []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2vec(myVocabList, postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0],\n",
       " [0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1],\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(trainMat))\n",
    "print(len(trainMat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(trainMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V,p1V,pAb=trainNB0(trainMat,listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03846154, 0.03846154, 0.03846154, 0.11538462, 0.03846154,\n",
       "       0.        , 0.        , 0.03846154, 0.03846154, 0.        ,\n",
       "       0.03846154, 0.03846154, 0.03846154, 0.        , 0.07692308,\n",
       "       0.        , 0.03846154, 0.03846154, 0.        , 0.        ,\n",
       "       0.03846154, 0.03846154, 0.03846154, 0.        , 0.03846154,\n",
       "       0.03846154, 0.        , 0.        , 0.03846154, 0.03846154,\n",
       "       0.        , 0.03846154])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(setOfWords2vec(myVocabList, listOPosts[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6546856420984069"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(array(setOfWords2vec(myVocabList, listOPosts[3]))*p0V)+log(pAb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04761905, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.04761905, 0.14285714, 0.        , 0.        , 0.04761905,\n",
       "       0.        , 0.04761905, 0.        , 0.04761905, 0.04761905,\n",
       "       0.04761905, 0.        , 0.        , 0.0952381 , 0.04761905,\n",
       "       0.        , 0.        , 0.        , 0.04761905, 0.        ,\n",
       "       0.0952381 , 0.04761905, 0.04761905, 0.        , 0.        ,\n",
       "       0.04761905, 0.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec,pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0 :\n",
    "        return 1\n",
    "    else :\n",
    "        return 0\n",
    "def testingNB():\n",
    "    listOPost,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2vec(myVocabList,postinDoc))\n",
    "    p0V,p1V,pAb =trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love','my','garbage']\n",
    "    thisDoc = array(setOfWords2vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as:',classifyNB(thisDoc, p0V,p1V,pAb))\n",
    "    testEntry = ['stupid','grabage']\n",
    "    thisDoc = array(setOfWords2vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as:',classifyNB(thisDoc, p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'garbage'] classified as: 0\n",
      "the word: grabage is not  in my  Vacobulary!\n",
      "['stupid', 'grabage'] classified as: 1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] +=1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    listOfTokens = re.split(r'/\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--- codeine 15mg -- 30 for $203.70 -- visa only!!! --\\n\\n-- codeine (methylmorphine) is a narcotic (opioid) pain reliever\\n-- we have 15mg & 30mg pills -- 30', '15mg for $203.70 - 60', '15mg for $385.80 - 90', '15mg for $562.50 -- visa only!!! ---']\n"
     ]
    }
   ],
   "source": [
    "wordList = textParse(open('email/spam/1.txt' ).read())\n",
    "print(wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList=[]\n",
    "    classList = []\n",
    "    fullText =[]\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    print(docList[5])\n",
    "    trainingSet = list(range(50)); testSet=[]          \n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in testSet:\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:       \n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print (\"classification error\",docList[docIndex])\n",
    "    print ('the error rate is: ',float(errorCount)/len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"what is going on there?\\ni talked to john on email.  we talked about some computer stuff that's it.\\n\\ni went bike riding in the rain, it was not that cold.\\n\\nwe went to the museum in sf yesterday it was $3 to get in and they had\\nfree food.  at the same time was a sf giants game, when we got done we\\nhad to take the train with all the giants fans, they are 1\", '2 drunk.']\n",
      "classification error [\"i've thought about this and think it's possible. we should get another\\nlunch. i have a car now and could come pick you up this time. does\\nthis wednesday work? 11:50?\\n\\ncan i have a signed copy of you book?\"]\n",
      "classification error ['hi peter,\\n\\nwith jose out of town, do you want to\\nmeet once in a while to keep things\\ngoing and do some interesting stuff?\\n\\nlet me know\\neugene']\n",
      "the error rate is:  0.2\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feedparser'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4b6f608be2f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mny\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeedparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://newyork.craigslist.org/stp/index.rss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'feedparser'"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "ny=feedparser.parse('http://newyork.craigslist.org/stp/index.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ny' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-e66f0716b197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mny\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entries'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mny\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entries'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ny' is not defined"
     ]
    }
   ],
   "source": [
    "ny['entries']\n",
    "len(ny['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList,fullText):\n",
    "    import feedparser\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "        sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1), reverse=True) \n",
    "        return sortedFreq[:30]        \n",
    "def localWords(feed1,feed0):\n",
    "    import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1) #NY is class 1\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabular\n",
    "    top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = range(2*minLen); testSet=[]           #create test set\n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print ('the error rate is: ',float(errorCount)/len(testSet))\n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 10)\n",
      "Help on class range in module builtins:\n",
      "\n",
      "class range(object)\n",
      " |  range(stop) -> range object\n",
      " |  range(start, stop[, step]) -> range object\n",
      " |  \n",
      " |  Return an object that produces a sequence of integers from start (inclusive)\n",
      " |  to stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\n",
      " |  start defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\n",
      " |  These are exactly the valid indices for a list of 4 elements.\n",
      " |  When step is given, it specifies the increment (or decrement).\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bool__(self, /)\n",
      " |      self != 0\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __reversed__(...)\n",
      " |      Return a reverse iterator.\n",
      " |  \n",
      " |  count(...)\n",
      " |      rangeobject.count(value) -> integer -- return number of occurrences of value\n",
      " |  \n",
      " |  index(...)\n",
      " |      rangeobject.index(value, [start, [stop]]) -> integer -- return index of value.\n",
      " |      Raise ValueError if the value is not present.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  start\n",
      " |  \n",
      " |  step\n",
      " |  \n",
      " |  stop\n",
      "\n"
     ]
    }
   ],
   "source": [
    "range(10)\n",
    "print(range(10))\n",
    "help(range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=[0]*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 50)\n"
     ]
    }
   ],
   "source": [
    "print(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(range(50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
